{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf6d4e5-0c50-4913-adbc-2abfa1eb6126",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ce000-52b2-4e17-9ce8-adf66afcfab3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We've slowly been moving parts of our code into files outside of notebooks... but *why*?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5770ebf-59e6-4aa4-b0b7-0f05a2058959",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- **Reuse** – we could copy `my_module.py` and use it in another project if we had need for the functions in there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6e7de-bb86-4a70-963a-68ac8182e154",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- **Testability** – one nice aspect of free-standing Python scripts is that we can write tests for them, checking that the functions inside are reliable and bug-free."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58011087-e74a-4561-b203-7da40d759c02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## The Value of Testing\n",
    "\n",
    "- By running your code on example inputs (for which you know the right output), you can be more confident that it will do what you expect\n",
    "\n",
    "- Since you may reuse code in other projects, it's smart to test on not just the data for the current project, but any inputs that your code might reasonably have to deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea80561-432d-469a-a2cc-8280e4e1d558",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "At this point, our directory setup is going to become very important, so let's take a quick detour to talk about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410baa11-5b62-471c-89f6-15e5fcd659bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "I'm going to be working with a project that looks something like this:\n",
    "```\n",
    "advanced-python-datasci/\n",
    "├── data/\n",
    "│   ├── adult-census.csv\n",
    "│   ├── ames.csv\n",
    "│   ├── ames_raw.csv\n",
    "│   └── planes.csv\n",
    "└── notebooks/\n",
    "    ├── 01-git.ipynb\n",
    "    ├── 02-explore_data.ipynb\n",
    "    ├── 03-first_model.ipynb\n",
    "    ├── 04-modular_code.ipynb\n",
    "    ├── 05-feat_eng.ipynb\n",
    "    ├── 06-model_eval.ipynb\n",
    "    ├── 07-modularity-pt2.ipynb\n",
    "    ├── 08-testing.ipynb\n",
    "    ├── 09-ml_lifecycle_mgt.ipynb\n",
    "    └── my_module.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1dd113-5a8c-4a1e-934a-a8022941f8bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "What's important:\n",
    "- At the top level, we have folders for `data` and `notebooks`\n",
    "- `my_module.py` is in our notebooks folder\n",
    "\n",
    "Take a few minutes to make sure your project repository is organized similarly. This will make a big difference in this section!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4a73bb-6f7d-4934-81e4-4eaf8c65986f",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "advanced-python-datasci/\n",
    "├── data/\n",
    "│   ├── adult-census.csv\n",
    "│   ├── ames.csv\n",
    "│   ├── ames_raw.csv\n",
    "│   └── planes.csv\n",
    "└── notebooks/\n",
    "    ├── 01-git.ipynb\n",
    "    ├── 02-explore_data.ipynb\n",
    "    ├── 03-first_model.ipynb\n",
    "    ├── 04-modular_code.ipynb\n",
    "    ├── 05-feat_eng.ipynb\n",
    "    ├── 06-model_eval.ipynb\n",
    "    ├── 07-modularity-pt2.ipynb\n",
    "    ├── 08-testing.ipynb\n",
    "    ├── 09-ml_lifecycle_mgt.ipynb\n",
    "    └── my_module.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef947385-c387-4ee1-a95e-804e7296c2af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## A Minimal Test\n",
    "\n",
    "- The easiest way to write a test is in a fresh Python script\n",
    "\n",
    "- Now that our project is organized, we can just create a new file in the `notebooks/` folder, called `tests.py`\n",
    "\n",
    "    - Remember that we can do this in Jupyter with File > New > Text File\n",
    "<br><br>    \n",
    "- Be sure this file appears in your notebooks folder!\n",
    "    - It's very important that it's in the same place as `my_module.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d2030-7521-4b0c-8f16-59323a5c3794",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Add the following code to your script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92c18669-cc07-44e3-b56d-ae6f2e80f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_module\n",
    "\n",
    "def test_invocation():\n",
    "    features, target = my_module.get_features_and_target(\n",
    "        csv_file='../data/adult-census.csv',\n",
    "        target_col='class'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf1d7a3-ac3a-4461-804e-86fb5f1538e3",
   "metadata": {},
   "source": [
    "<div class=\"admonition note alert alert-info\">\n",
    "    <b><p class=\"first admonition-title\" style=\"font-weight: bold;\">Discussion</p></b>\n",
    "    If we were to run this code on its own with <code>python tests.py</code>, what would happen?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fd803c-18cc-46f5-9c05-aeae3e44fd3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Running Our Test\n",
    "- We're going to invoke our test with **pytest**, a tool we'll discuss more shortly\n",
    "- Open a terminal session (in Jupyter, File > New > Terminal)\n",
    "    - Things in the terminal are a bit different here in Windows vs Mac/Linux, so we'll try to help how we can..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b220f1-903e-4e87-915e-b69cc27f7723",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Run the below command in a notebook to find out what folder you're currently working in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeeb56f0-f89e-4df2-a3a0-5e0c822d03b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/eswan18/Teaching/advanced-python-datasci/notebooks'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5524d865-3db0-421f-a730-2b20190586c9",
   "metadata": {},
   "source": [
    "Copy that result (including the quotes) and in your terminal, paste it after the `cd` command.\n",
    "\n",
    "So in my terminal, I would run:\n",
    "```bash\n",
    "cd '/Users/eswan18/Teaching/advanced-python-datasci/notebooks'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1e8ba-c980-498b-8d8d-9cb723d469e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now...\n",
    "- Windows users: run `dir`\n",
    "- Mac/Linux users: run `ls`\n",
    "\n",
    "This lists the contents of the folder you're currently inside.\n",
    "You should see `my_module.py` and `tests.py` among the output.\n",
    "\n",
    "![ls](images/ls.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7274f-a733-43e5-96e9-46b5fc26c9c8",
   "metadata": {},
   "source": [
    "Now, we're almost ready to run our test.\n",
    "The only thing left is to set up our terminal so that it's using the same Conda environment as our notebooks -- because `pytest` is installed in that environment.\n",
    "- If you took the intermediate class with us, we discussed Conda and environments in more detail then."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3914bfde-a3e0-4812-830e-c94709b0f7ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In the terminal, run\n",
    "```bash\n",
    "conda activate uc-python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bcd1b7-c0b6-40ca-84d8-1e5579ed694a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "This should add a \"uc-python\" prefix to your terminal prompt:\n",
    "![ls](images/prefix-prompt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b4577-bce3-4cf6-bfcd-3d37daacadaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "Note that your prompt will look quite a bit different from mine; all that matters is the folder name and the \"uc-python\" prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274568fe-5dd9-4545-a204-1db88791b57d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now we're ready to run our test!\n",
    "\n",
    "In your terminal, type:\n",
    "```bash\n",
    "pytest tests.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c4a05-10b8-492a-9fd5-8ac26d7ee1fe",
   "metadata": {},
   "source": [
    "You should see some output appear.\n",
    "The last line should look something like:\n",
    "```\n",
    "============ 1 passed in 0.74s ============\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faccf693-ed0b-4fd3-981c-10af87607206",
   "metadata": {},
   "source": [
    "This means that 1 (test) passed and 0 tests failed, and the whole process took 0.74 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c460a17-17ae-4da5-925b-24c16ad687fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Pytest\n",
    "\n",
    "- Pytest is an automated tool for running sets of tests\n",
    "  - sets of tests are often called test \"suites\"\n",
    "- It expects your tests to be in their own files, and each test needs to be a function\n",
    "  - The name of each function must start with `test_`, so pytest knows it's a test and not just a regular function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecfd6d5-ed3c-4f31-9266-5fe7025a0a43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's look back at our simple test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31aa8a4-edc4-4fa7-9cbd-ea9bd8f4758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_module\n",
    "\n",
    "def test_invocation():\n",
    "    features, target = my_module.get_features_and_target(\n",
    "        csv_file='../data/adult-census.csv',\n",
    "        target_col='class'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7209e5d-2a05-4b5f-8abe-cba67072dcda",
   "metadata": {},
   "source": [
    "Note that our function starts with `def test_`, and pytest is smart enough to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd2e226-626c-4fea-b981-9a58b66bf3ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "What happens if a test fails?\n",
    "Let's add a bad test just to see.\n",
    "\n",
    "Add this function to `tests.py`, below `test_invocation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e546d1e1-975e-4efd-aad1-2aff599e5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_without_args():\n",
    "    # A test we know will fail because we don't provide arguments\n",
    "    # to the function.\n",
    "    features, target = my_module.get_features_and_target()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d3ea93-1f8c-4c77-a513-7791d113cf5c",
   "metadata": {},
   "source": [
    "Save the file, and then rerun `pytest tests.py` in your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9804ce-445f-48bc-9517-7cbdcfde4e71",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "============== 1 failed, 1 passed in 0.83s =============="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a9114-8504-4982-8bd8-837f58839477",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Our original test still passes, but this one fails!\n",
    "\n",
    "Above this line, pytest reports exactly what happened that caused it to fail.\n",
    "We got an error:\n",
    "```text\n",
    "    def test_without_args():\n",
    "        # A test we know will fail because we don't provide arguments\n",
    "        # to the function.\n",
    ">       features, target = my_module.get_features_and_target()\n",
    "\n",
    "E       TypeError: get_features_and_target() missing 2 required positional arguments: 'csv_file' and 'target_col'\n",
    "\n",
    "tests.py:12: TypeError\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd32840e-ad8f-41e4-bb68-7e960d467419",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## What does it mean to \"fail\"?\n",
    "\n",
    "- If a test function encounters any kind of unexpected error, that counts as a failure to pytest\n",
    "- Any test that runs without error \"passes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39977b2c-09f9-4dff-8ca0-5561c45a70e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's remove our `test_without_args` test -- it's not something we actually want to verify about our code.\n",
    "\n",
    "However, one thing we *do* want to check is that the features and target that are returned from our function are a pandas DataFrame and Series, respectively. Let's add a test for that in its place..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07651ef2-dbe0-41df-86c8-ea7a3ca24518",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd # You may want to move this import to the top of the file.\n",
    "\n",
    "def test_return_types():\n",
    "    features, target = my_module.get_features_and_target(\n",
    "        csv_file='../data/adult-census.csv',\n",
    "        target_col='class'\n",
    "    )\n",
    "    assert isinstance(features, pd.DataFrame)\n",
    "    assert isinstance(target, pd.Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698f939-bb0e-437a-a4dc-0e187a4c901a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The we can rerun `pytest tests.py`\n",
    "```text\n",
    "=========== 2 passed in 0.88s ===========\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebfc8fa-4394-46e0-aaad-73e114df44fb",
   "metadata": {},
   "source": [
    "Nice! It looks like our function does indeed return a DataFrame and a Series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a6d26-3306-485a-9acc-ec3f0ed5f2b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Assert\n",
    "\n",
    "- We used the `assert` keyword to check that `features` was a DataFrame\n",
    "- `assert` is a special Python feature that raises an error if the expression after it isn't True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9d6ee71-8e4b-4999-b40c-8b05c624d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 3 - 2 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d237009-7bc5-4fa6-8c04-22506209ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 100 > 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faf14df1-233c-478b-95de-a955b21bb15d",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j3/v1318ng94fvdpq7kzr0hq9kw0000gn/T/ipykernel_72985/55912000.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 4 * 3 == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6785b723-dcc9-4086-b206-2e4d0cb39254",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "One common use of `assert` in tests is to check that a variable contains a certain kind of object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56743faa-7b87-46f9-b1a5-fe7cb3e937f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5\n",
    "assert isinstance(x, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be034bab-458d-4171-a020-698e2b49ba46",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j3/v1318ng94fvdpq7kzr0hq9kw0000gn/T/ipykernel_72985/3799747894.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert isinstance(x, str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff577f-c12b-4173-98be-03f8a05ebc16",
   "metadata": {},
   "source": [
    "But you can use assert to check any expression in Python that evaluates `True` or `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c44769-a2e3-46ef-92f4-ab19d8084638",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "What would happen if we had expected `target` to be a `list` instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ec64358-8087-4140-9ef7-3aed0078b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_return_types():\n",
    "    features, target = my_module.get_features_and_target(\n",
    "        csv_file='../data/adult-census.csv',\n",
    "        target_col='class'\n",
    "    )\n",
    "    assert isinstance(features, pd.DataFrame)\n",
    "    assert isinstance(target, list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5668c941-3bab-43fe-8244-4153d10f7de2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![Assert False](images/assert-false.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a3e97-05c1-467a-9a4f-7c92c2d16db5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "These kinds of tests are handy, because we can make sure our functions return the types of things we expect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dd83ec-ac6b-4ea2-97b3-632cc3a607cb",
   "metadata": {},
   "source": [
    "<div class=\"admonition note alert alert-info\">\n",
    "    <b><p class=\"first admonition-title\" style=\"font-weight: bold;\">Discussion</p></b>\n",
    "    What other aspects of <code>get_features_and_target</code> might we want to test?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b129f-31f9-4285-b627-c9fc0ee6bf33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## `test_cols_make_sense`\n",
    "\n",
    "- If `get_features_and_target` works as we expect, the `target` should be a column in the DataFrame, as should *each* of the columns in `features`.\n",
    "- Here's a test to check that.\n",
    "  - There's some pandas functionality in here that we haven't discussed yet, but it should be clear what's happening.\n",
    "- Add this test to `tests.py` at the bottom. Make sure you're importing `pandas` somewhere in the file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b494430-528a-4d2c-b31d-a95f79803a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cols_make_sense():\n",
    "    features, target = my_module.get_features_and_target(\n",
    "        csv_file='../data/adult-census.csv',\n",
    "        target_col='class'\n",
    "    )\n",
    "    # Load the data ourselves so we can double-check the columns\n",
    "    df = pd.read_csv('../data/adult-census.csv')\n",
    "    assert target.name in df.columns\n",
    "    # Use a list comprehension to check all the feature columns\n",
    "    assert all([feature_col in df.columns for feature_col in features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff541ec-fe8a-4d6d-85a8-3a261ed851ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "```text\n",
    "============= 3 passed in 0.97s =============\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b8dd44-6763-46b1-a7a2-25117f8fe3ad",
   "metadata": {},
   "source": [
    "Whoo!\n",
    "`get_features_and_target` looks pretty reliable;\n",
    "I feel more comfortable using it across modeling projects to load data and split it into a target and a DataFrame of numeric features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a7d24-d409-431e-96e9-d33b8f7351cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "If I were planning to use it more, though, there are some other things I might think about testing:\n",
    "- The number of elements in `target` should match the number of rows in the original data, as should the number of rows in `features`.\n",
    "- All of the columns in `features` should be numeric.\n",
    "- All numeric columns in the input file should be present either in `features` or `target`\n",
    "- The name of the `target` series should match the `target_col` argument that we passed into the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202abf96-24ac-4726-9b40-6836834636b8",
   "metadata": {},
   "source": [
    "But for the sake of time, we're going to stop at these three tests for the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb04c85-4769-46eb-af95-169d64c0339e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Parametrization\n",
    "- Another way we could check the robustness of `get_features_and_target` is by making sure that it works on multiple data sets, not just the adult census data.\n",
    "- We could write entirely new tests for this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "287234a2-cf40-4df4-a4b0-2d95bce6d522",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_return_types_census():\n",
    "    features, target = my_module.get_features_and_target(\n",
    "        csv_file='../data/adult-census.csv',\n",
    "        target_col='class'\n",
    "    )\n",
    "    assert isinstance(features, pd.DataFrame)\n",
    "    assert isinstance(target, pd.Series)\n",
    "    \n",
    "def test_return_types_ames():\n",
    "    features, target = my_module.get_features_and_target(\n",
    "        csv_file='../data/ames.csv',\n",
    "        target_col='Sale_Price'\n",
    "    )\n",
    "    assert isinstance(features, pd.DataFrame)\n",
    "    assert isinstance(target, pd.Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cf1562-fbba-4a05-b236-4b4891cd4e7a",
   "metadata": {},
   "source": [
    "But this is very duplicative. There has to be a better way!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec3ffc-a03d-4ec0-b3d4-c4693efb2179",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Indeed, there is: it's **parametrization**.\n",
    "\n",
    "To parametrize a test is to give it multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c01dac0-6a9c-49e2-9af8-22684a0cc158",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    'csv,target',\n",
    "    [\n",
    "        ('../data/adult-census.csv', 'class'),\n",
    "        ('../data/ames.csv', 'Sale_Price')\n",
    "    ]\n",
    ")\n",
    "def test_return_types(csv, target):\n",
    "    features, target = my_module.get_features_and_target(\n",
    "        csv_file=csv,\n",
    "        target_col=target\n",
    "    )\n",
    "    assert isinstance(features, pd.DataFrame)\n",
    "    assert isinstance(target, pd.Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf0ca59-ea2e-4cfe-980f-1f1cb74d70bb",
   "metadata": {},
   "source": [
    "Notice that our inputs are parameters to our test function.\n",
    "`@pytest.mark.parametrize` takes a list of tuples to be passed to our test, along with the arguments they correspond to (`'csv,target'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc51e081-17db-4cef-aa9d-ea64a18854c5",
   "metadata": {},
   "source": [
    "This syntax may look complicated initially, but it's easy enough to copy, paste, and modify for each test you want to parametrize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2277e9-12ac-40ff-b70c-3039ce708bf3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "```text\n",
    "=========== 4 passed in 0.93s ==========\n",
    "```\n",
    "\n",
    "This test should pass, and notice that we went from having 3 tests to 4 -- because one of them is being run twice, once for each parameter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uc-python",
   "language": "python",
   "name": "uc-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
